---
title: 'Homework: Finite Precision Arithmetic & Gradient Descent'
output:
  html_notebook:
    code_folding: none
    highlight: textmate
---


```{r setup, include=FALSE}
source(file.path("..", "R", "colors.R"))
source(file.path("..", "R", "util.R"))

required_packages <- c('Matrix', 'testthat')
install_and_load_packages(required_packages)
```

# Exercise 0: Checking your understanding of floating point numbers and finite precision arithmetic

We observed in the class that the following code returns `FALSE`:

```{r, eval=FALSE}
a <- 1 + .Machine$double.eps
b <- 1 + .Machine$double.eps 
c <- 1
(a + b) + c == a + (b + c)
```

On the other hand, the following modification results in `TRUE`:

```{r, eval=FALSE}
b <- 0.5 + .Machine$double.eps
(a + b) + c == a + (b + c)
```

Why do we get the different result?

**My answer:** We get different results here because of the limitations of doing finite precision arithmetic with floating point numbers and machine epsilon (i.e., `.Machine$double.eps` in R). Machine epsilon is defined as the limit of relative approximation and is approximately equal to $2^{-52}$ on most computers. This quantity will be written mathematically below as the symbol $\epsilon$.

In the first example, `a` and `b` are both equal to `1 + .Machine$double.eps`, which is slightly greater than 1. When we add `a` and `b` together, the result is a number that is slightly greater than 2 (`2 + 2*.Machine$double.eps`). When this number is added to `c` (which is equal to 1), we get a number that is slightly greater than 3 (`3 + 2*.Machine$double.eps`). On the other hand, when we add `a` to `b + c`, the result is exactly 3 because `1 + .Machine$double.eps` and `2 + .Machine$double.eps` are indistinguishable (for the computer) from 1 and 2, respectively. Hence, `(a + b) + c` is not equal to `a + (b + c)`, and the expression returns `FALSE`. This paragraph is also written in mathematical notation below:

LHS:
$$(a+b)+c \to (1+\epsilon+1+\epsilon)+1 \to (2+2\epsilon)+1 \to 3+2\epsilon.$$
RHS:
$$a+(b+c) \to 1+\epsilon+(1+\epsilon+1) \to 1+\epsilon+(2+\epsilon) \to 1+\epsilon+(2) \to 1+(2) \to 3.$$

Hence:
$$3+2\epsilon \neq 3.$$

In the second example, `b` is equal to `0.5 + .Machine$double.eps`, which is slightly greater than 0.5. When we add `a` (which is slightly greater than 1) and `b`, the result is a number that is slightly greater than 1.5. This number is still representable with floating-point precision when it is added to `c` (which is 1), and we get a number that is slightly greater than 2.5 (`2.5 + 2*.Machine$double.eps`). Similarly, when we add `a` to `b + c`, the result is a number that is slightly greater than 2.5 (`2.5 + 2*.Machine$double.eps`) since `b + c` (which is `1.5 + .Machine$double.eps`) is representable with floating-point precision when it is added to `a` (which is `1 + .Machine$double.eps`). Hence, `(a + b) + c` is equal to `a + (b + c)`, and the expression returns `TRUE`. This paragraph is also written in mathematical notation below:

LHS:
$$(a+b)+c \to (1+\epsilon+0.5+\epsilon)+1 \to (1.5+2\epsilon)+1 \to 2.5+2\epsilon.$$

RHS:
$$a+(b+c) \to 1+\epsilon+(0.5+\epsilon+1) \to 1+\epsilon+(1.5+\epsilon) \to 2.5+2\epsilon.$$

Hence:
$$2.5+2\epsilon = 2.5+2\epsilon.$$

In summary, this exercise demonstrates that addition with floating point numbers and machine epsilon is not associative, and the order of addition operations can make a difference. 

# Exercise 1: One-pass algorithm for calculating variance

Let's say you are running a Monte Carlo simulation to estimate the mean and variance of some random variable $\boldsymbol{\theta}$. 
Imagine that $\boldsymbol{\theta}$ is high-dimensional, so you don't want to keep all the Monte Carlo samples $\boldsymbol{\theta}_i$ due to the memory burden.
(If $\boldsymbol{\theta}$ is $10^6$-dimensional, for example, storing 1,000 Monte Carlo samples would require 8GB of memory.)
And it occurs to you that you can write a "one-pass" algorithm to calculate the sample mean and sample second moment of $\boldsymbol{\theta}$ by only keeping track of running totals of Monte Carlo samples. 
You can then, in theory, calculate the sample variance from these quantities.

The above idea is implemented and applied below. 
Try it and see what you get. 
Does the estimate make sense? If not, why?
How does the estimate change if you change the seed?

**Bonus question:** 
What is, roughly speaking, the distribution of the estimator `sample_var_with_question_mark` under repeated Monte Carlo simulations (i.e. without a fixed seed)?

```{r}
set.seed(140778)

running_sum <- 0
running_sum_sq <- 0

n_sample <- 10^3
for (i in 1:n_sample) {
  theta <- 1e10 + rnorm(1)
  running_sum <- running_sum + theta
  running_sum_sq <- running_sum_sq + theta^2
}
sample_mean <- running_sum / n_sample
sample_sec_mom <- running_sum_sq / n_sample

sample_var_with_question_mark <- (sample_sec_mom - (sample_mean)^2)

# My code (appended to the end of the original script above)
print(paste0("The sample mean is: ", sample_mean))
print(paste0("The sample second moment is: ", sample_sec_mom))
print(paste0("The sample variance (with question mark) is: ", sample_var_with_question_mark))
```

## Things to commit/submit
Answers to the questions above.

**My answers to the questions above:** From running the script above, we find that the sample mean is \~1e10 (9999999999.97963), the sample second moment is \~1e20 (99999999999592366080), and the sample variance (with question mark) is -147456.

The sample mean makes sense and aligns with our expectations since the variable being simulated from (`theta`) is normally distributed with mean 1e10 and variance 1. The sample second moment is also in line with the theoretical value of the variance plus the square of the mean for this normal distribution (i.e., 1e20 + 1).

However, the calculated sample variance (with question mark) is -147456, which is very negative (i.e., not possible for a variance estimate, as the variance is always non-negative for non-deterministic random variables). This discrepancy is likely due to the large mean of the target distribution and the subsequent calculations. When we square the large values generated for the estimation of the second moment (in calculating `running_sum_sq`) and square the mean calculated from the running first-moment sum (in calculating `sample_var_with_question_mark`), the deviations from the actual mean are greatly amplified. This results in a high variability in both the second moment estimate and the squared first moment estimate, leading to the calculated sample variance deviating significantly from the actual variance of 1.

If we change the seed (as demonstrated below), we observe that the sample mean and sample second moment are \~1e10 and \~1e20, respectively, but the sample variance is still unreasonably large (524288).

```{r}
set.seed(140850)

running_sum <- 0
running_sum_sq <- 0

n_sample <- 10^3
for (i in 1:n_sample) {
  theta <- 1e10 + rnorm(1)
  running_sum <- running_sum + theta
  running_sum_sq <- running_sum_sq + theta^2
}
sample_mean <- running_sum / n_sample
sample_sec_mom <- running_sum_sq / n_sample

sample_var_with_question_mark <- (sample_sec_mom - (sample_mean)^2)

print(paste0("The sample mean is: ", sample_mean))
print(paste0("The sample second moment is: ", sample_sec_mom))
print(paste0("The sample variance (with question mark) is: ", sample_var_with_question_mark))
```

**My bonus question answer:** Since each sampled `theta` is assumed to be iid normal with mean 1e10 and variance 1, a linear combination of 1,000 of these samples (i.e., the sample mean) is normally distributed as well with mean 1e10 and variance 1e-3. 

Recall that the sum of $n$ iid squared normal random variables, each with mean $\mu$ and variance 1, has a chi-squared distribution with degrees of freedom $n$ and noncentrality parameter $\lambda = n\mu^2$. Using this fact, we find that 1e3 times the sample mean squared has a chi-squared distribution with $n$ = 1 degree of freedom and noncentrality parameter $\lambda$ = 1 $\times$ (1e13)$^2$ =  1e26. Hence, the sample mean squared has this chi-squared distribution scaled by 1e3, and the running sum of squares has a chi-squared distribution with $n$ = 1e3 degrees of freedom and noncentrality parameter $\lambda$ = 1e3 $\times$ (1e10)$^2$ = 1e23. 

Consequently, the distribution of the sample variance estimator `sample_var_with_question_mark`, when obtained from repeated Monte Carlo simulations without a fixed seed, follows a generalized chi-squared distribution because it is a weighted sum of random variables that follow non-central chi-square distributions.

# Exercise 2: Generating random real numbers in finite precision arithmetic

## Part A

You learned in the class that numeric values in computers are typically represented with 64-bits double-precision numbers, having 52-bits fraction. 
In particular, a "Uniform(1, 2)" random variable in double-precision generates a number from the $2^{52}$ unique values between $1$ and $2$.
The amount of randomness in it, therefore, is equivalent to 52 Bernoulli random variables.
Write a function to generate a Uniform(1, 2) random variable in double-precision with randomness coming only from the `rbinom()` function.
Check the function's behavior by plotting a histogram of random variables generated from it. 

```{r, eval=TRUE}
double_prec_runif <- function () {
  # Generate 52 random bits using the `rbinom()` function 
  bits <- rbinom(52, 1, 0.5)
  # Convert these bits into a fraction in [0,1) 
  bits_fraction <- sum(bits*2^(-(1:52)))
  # Add 1 to the fraction to get a number in [1,2)
  return(1 + bits_fraction)
}

n_draw <- 10^5
unif_rv <- sapply(1:n_draw, function (i) double_prec_runif())

rv_hist <- hist(unif_rv, breaks = 21, plot = FALSE)
plot(
  rv_hist,
  freq = FALSE,
  xlab = "Realized values",
  col = "#bdcedb", # JHU Spirit Blue with saturation and luminance adjusted: https://hslpicker.com/#bdcedb
  border = "#859bad" # Another variation of JHU Spirit Blue
)
```

## Part B

As we observed above, a Uniform(1, 2) random variable in double-precision takes one of the $2^{52}$ unique values.
Two independently drawn random variables, therefore, has small but positive probability of having exactly the same value.
We can calculate the expected number of duplicate values in i.i.d. Uniform(1, 2) random variables as follows.
When drawing with replacement $d$ independent random variables out of $n$ choices, the probability of the $i$-th element never selected is $(1 - n^{-1})^d$ for $i = 1, \ldots, n$. 
The expected number of unique items drawn, therefore, is $n [1 - (1 - n^{-1})^d]$.
In turn, the expected number of duplicates is $d - n [1 - (1 - n^{-1})^d]$.
For the case of double-precision uniform random variables, we have $n = 2^{52}$ for the number of choices.

The `calc_num_expected_duplicate()` function below implements the above formula for the expected number of duplicates. 
When you call `calc_num_expected_duplicate(n_draw = 10^6, n_choice = 2^52)` to calculate the expected number for $10^6$ Uniform(1, 2) random variables, however, the function returns _exact zero_ due to a finite precision error. 
What is the issue?

```{r}
calc_num_expected_duplicate <- function (n_draw, n_choice) {
  n_log_expected_duplicate <- log(n_draw) + log(n_draw - 1) - log(n_choice) - log(2)
  n_expected_duplicate <- exp(n_log_expected_duplicate)
  return(n_expected_duplicate)
}
```

Formulas involving $1 + x$ for $|x| \ll 1$ ($x = - n^{-1}$ in the above case) occur commonly enough in statistics/scientific computing that R provides a function `log1p(x)` for computing $\log(1 + x)$ to deal with one such situation.
In more general calculations involving $|x| \ll 1$ (e.g. for a "sinc" funciton $\operatorname{sinc}(x) = \sin(x) / x$), a Taylor expansion provides a general approach to control truncation errors.

Remedy the `calc_num_expected_duplicate()` function by applying the second-order Taylor expansion to $(1 - x)^d$ and simplifying the formula $d - x^{-1} [1 - (1 - x)^d]$ to avoid truncation errors. 
(Remark: i) in this specific case, you could use the binomial expansion and calculate it exactly; 
ii) applying the second-order expansion to $(1 - x)^d$ corresponds to applying the first-order expansion to the overall expression $d - x^{-1} [1 - (1 - x)^d]$.)
Use the more numerically accurate version of `calc_num_expected_duplicate()` to calculate the expected number of duplicates among $10^6$ Uniform(1, 2) random variables.

## Part C

The expected number from Part B, if you calculated it correctly, suggests that it is extremely rare to get duplicate values when generating $10^6$ uniform random variables in double-precision.
This is reassuring because we routinely generate a far larger number of random numbers in modern applications.
If we use R's `runif()` function, however, this is _not_ what we observe because [R's random numbers are generated using 32-bit integers](https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1782261).
Check the number of duplicates from `runif(10^6)` against the expected value for a floating-point number with 32-bits of precision. 
Check also that, using your implementation of random uniform from Part A, duplicates are indeed rare in double-precision.

```{r, eval=TRUE}
n_draw <- 10^6
base_r_unif <- runif(n_draw, min = 1, max = 2)
unif_rv <- sapply(1:n_draw, function (i) double_prec_runif())
# Check the number of duplicates against the expected value
print(paste0("The number of duplicates from runif(10^6) is: ", n_draw - length(unique(base_r_unif))))
print(paste0("The expected number of duplicates from runif(10^6) is: ", calc_num_expected_duplicate(n_draw = n_draw, n_choice = 2^(32))))
print(paste0("The number of duplicates from double_prec_runif() is: ", n_draw - length(unique(unif_rv))))
print(paste0("The expected number of duplicates from double_prec_runif() is: ", calc_num_expected_duplicate(n_draw = n_draw, n_choice = 2^(52))))
```


## Things to commit/submit
Answers to the tasks/questions above.

**My answer to Part A:** Please see the code and histogram above under "Part A." The code generates a histogram of 100,000 random variables generated by the `double_prec_runif()` function. This histogram is approximately uniform, which demonstrates that the function is correctly generating random numbers in the interval [1,2).

**My answer to Part B:** The issue with the `calc_num_expected_duplicate()` function is that it suffers from a loss of precision due to subtractive cancellation. This happens when we subtract two nearly equal numbers, which leads to a loss of significant digits. In this case, when `n_draw` is large and `n_choice` is much larger (as in the case of double-precision floating point numbers where `n_choice = 2^52`), both 1 and `1 - 1 / n_choice` are very close to each other, and raising them to the power of `n_draw` makes them even closer. When these two close numbers are subtracted from each other, the result is a number that is much smaller than either of the original numbers, and a lot of the significant digits are lost in the process.

Suppose that $f(x) = x^d$. Then the second-order Taylor expansion of $f(\cdot)$ around $x=1$ is:
$$f(x+1) \approx f(1) + f'(1)x + f''(1)\frac{x^2}{2} = 1 + dx + d(d-1)\frac{x^2}{2}.$$

Applying this result to the given formula (in the instructions for Part B) for the expected number of duplicates, we have:
$$
\begin{align*}
d - n \left[1 - \left( 1 - \frac{1}{n} \right)^d \right] &= d - n + n \left(1 - \frac{1}{n}\right)^d \\
  &\approx d - n + n\left(1 - \frac{d}{n} + d(d-1)\frac{1}{2n^2}\right) \\
  &= d - n + n - d + d(d-1)\frac{1}{2n} \\
  &= d(d-1)\frac{1}{2n}.
\end{align*}
$$

We can then use a log transformation to further simplify the formula above and increase our numerical stability.
$$d - n \left[1-\left(1-\frac{1}{n}\right)^d\right] \approx d(d-1)\frac{1}{2n} =\exp\left[\log(d) + \log(d-1) - \log(n) - \log(2)\right].$$

Please see the remedied function above (under "Part B") that uses this result. 

Here, we use this function to calculate the expected number of duplicates among $10^6$ Uniform(1, 2) random variables:
```{r}
n_duplicates <- calc_num_expected_duplicate(n_draw = 10^6, n_choice = 2^52)
print(n_duplicates)
```

**My answer to Part C:** Please see the code above (under "Part C") for my implementations of the two requested checks. The number of duplicates from `runif(10^6)` (120) is close to the expected value for a floating-point number with 32-bits of precision (116.415205411613). Similarly, using my implementation of random uniform from Part A, duplicates are rare in double-precision; the number of duplicates (0) is very close to the expected value of 0.000111022191440213. 

# Exercise 3: Numerical differentiation

## Part A: Potential caveats of numerical differentiation

You have learned in the class that the centered difference $\frac{f(x + \Delta x) - f(x - \Delta x)}{2 \Delta x}$ approximates the derivative $f'(x)$ up to an $O(\Delta x^2)$ error.
To numerically verify this, we design the following experiment using a log of logistic function (i.e. $f(x) = \log\{ \exp(x) / (1 + \exp(x)) \}$) as an example.
We evaluate its derivative using an analytical formula and compare it to the centered difference approximations as $\Delta x$ varies from $2^{-1}$ to $2^{-52}$.
Using the starter code provided below, plot the relative error of the numerical approximation in a log-log scale.
What do you expect the slope of the plotted line to be?

```{r, eval=FALSE}
#' Approximate derivative via centered-difference approximation
approximate_deriv <- function(func, x, dx) {
  # Fill in
}

log_logistic_func <- function(x) {
  # Fill in
}

log_logistic_deriv <- function(x) {
  # Fill in
}

# Calculate and plot the relative errors of finite difference approximations
set.seed(615)
x <- rnorm(1)

log2_dx <- - seq(1, 52)
numerical_deriv <- sapply(
  2^log2_dx, function(dx) approximate_deriv(log_logistic_func, x, dx)
)
analytical_deriv <- log_logistic_deriv(x)
rel_err <- abs(
  (analytical_deriv - numerical_deriv) / analytical_deriv
)

fontsize <- 1.2
plot(
  log2_dx, log2(rel_err),
  frame = FALSE, # Remove the ugly box
  col = jhu_color$spiritBlue,
  xlab = "log2 stepsize for finite difference",
  ylab = "log2 rel errors",
  xlim = rev(range(log2_dx)),
  cex.lab = fontsize,
  cex.axis = fontsize,
  type = 'l'
)

```

**Questions:**
Does the relative error plot agree with what the mathematical theory predicts? Explain the observed phenomenon.

Also, what is the slope in the "latter part" of your plotted line, roughly speaking? 
Explain why we observe such a slope in the relative error. 
(Hint: Consider a machine with 10 decimal digits of accuracy. What happens if we numerically differentiate $f(x) = x$ at $x = 1$ with $\Delta x = 0.1, 0.01, 0.001, \ldots$.)

## Part B: Extention to multivariate functions

As mentioned in the lecture, the finite difference method extends to a function with multivariate input $f: \mathbb{R}^d \to \mathbb{R}$ and can be used to approximate the gradient $\nabla f(\boldsymbol{x})$.
The extension relies on the fact $f(\boldsymbol{x} + \Delta x \, \boldsymbol{e}_i) = f(\boldsymbol{x}) + \Delta x \, \partial_i f(\boldsymbol{x}) + \ldots$ for $i = 1, \ldots, d$.

Using the starter code provided below, implement a function to approximate $\nabla f(\boldsymbol{x})$ via the centered difference method.
To test this function, take as an example a quadratic function $f(\boldsymbol{x}) = - \boldsymbol{x}^\intercal \boldsymbol{\Phi} \boldsymbol{x} / 2$ with positive definite $\boldsymbol{\Phi}$, which can be thought of as a log-likelihood function for $\boldsymbol{x} \sim \mathscr{N}(\boldsymbol{0}, \boldsymbol{\Phi})$ up to an additive constant.
First show that its gradient is given by $\nabla f(\boldsymbol{x}) = - \boldsymbol{\Phi} \boldsymbol{x}$; 
more generally, the gradient of a quadratic function $g(\boldsymbol{x}) = \boldsymbol{x}^\intercal \boldsymbol{A} \boldsymbol{x}$ is given by $\nabla g(\boldsymbol{x} = (\boldsymbol{A} + \boldsymbol{A}^\intercal) \boldsymbol{x}$ for any square matrix $\boldsymbol{A}$.
Then compare a numerically approximated gradient to one computed via an analytical formula.

```{r, eval=FALSE}
approx_grad <- function(func, x, dx = .Machine$double.eps^(1/3)) {
  numerical_grad <- rep(0, length(x))
  # Fill in
  return(numerical_grad)
}

set.seed(410)
n_param <- 4
X <- matrix(rnorm(2 * n_param^2), nrow = 2 * n_param, ncol = n_param)
Sigma_inv <- t(X) %*% X
  
#' Calculate log-density of centered Gaussian up to an additive factor
gaussian_logp <- function(x, Sigma_inv) {
  logp <- - .5 * t(x) %*% Sigma_inv %*% x
  return(logp)
}

gaussian_grad <- function(x, Sigma_inv) {
  grad <- rep(0, length(x)) # Replace with an actual formula
  return(grad)
}

x <- c(3, 1, 4, 1)

analytical_grad <- gaussian_grad(x, Sigma_inv)
numerical_grad <- approx_grad(function(x) gaussian_logp(x, Sigma_inv), x)
testthat::expect_true(are_all_close(
  analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
```


## Things to commit/submit
For Part A, completed code and answers to the questions.
For Part B, derivation of the gradient and completed code that passes the test.


# Exercise 4: Continuing development of `hiperglm` &mdash; finding MLE via `stats::optim()` and testing it

We will continue the development of the `hiperglm` package. 

As discussed, our plan is to first implement an MLE finder via a pre-packaged general-purpose BFGS optimizer, against which we can then use to benchmark our custom implementation of iteratively reweighted least squares. 
The idea is that using the pre-packaged function is less error-prone, especially since the gradient function is easy to unit-test.

To be extra safe, we are going to test the MLE finder via BFGS against an even simpler algorithm: the least square for a linear model via the analytical expression $\hat{\boldsymbol{\beta}}_\textrm{mle} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}$.
Complete the following steps toward this goal.
Remember that you can use [`devtools::load_all()`](https://r-pkgs.org/code.html#sec-code-load-all) to help your [package development process](https://r-pkgs.org/code.html#constant-health-checks). 

0. Make and check out (`git branch` and `git checkout`) a development branch (call it `develop`) where you will commit the work below.
1. Set up the use of `testthat`.
2. Copy the helper functions `are_all_close()` and `simulate_data()` from [the instructor's repo](https://github.com/aki-nishimura/hiperglm/blob/3706c61e1492fe8ccc77e414626d43219b65e659/tests/testthat/helper.R).
3. Test the `are_all_close()` function. 
Write a test to cover at least three cases: the function 1) correctly returns `TRUE`, 2) correctly returns `FALSE` because the relative error is above `rel_tol`, and 3) correctly returns `FALSE` because the absolute error is above `abs_tol`. 
Make sure that the logic of the test is clear and easy to understand &mdash; tests are there to clarify and validate the code's behavior; unreadable tests defeat the purpose.
<br> _Remark:_ This is just an exercise, but in general there is nothing strange about testing a function you use to test another function. It is perfectly reasonable to consider testing any functions complicated enough to potentially contain insiduous bugs.
4. Set up the (currently failing) test to compare the MLE estimated via pseudo-inverse and via BFGS. 
5. Implement the MLE finder via peudo-inverse. 
Please, pretty please, do not calculate an inverse of a matrix. 
(Otherwise, it's fine to use `solve()` instead of `chol()`.)
6. Implement the MLE finder via BFGS:
    a. Implement functions to calculate the log-likelihood and gradient under a linear model. 
    This function technically requires an estimate of the residual variance, but its value does not affect the MLE for the coefficients and hence is irrelevant for our purpose. 
    You can just specify it as an optional parameter e.g. as `function(..., noise_var = 1)`.
    b. Test your gradient calculation by comparing it against a numerical one.
    For this purpose, use the function from Exercise 2, Part B to approximate the gradient of a given function via finite difference.
    <br> _Remark:_ Technically, this test checks only for consistency between the log-likelihood and gradient function.
    It is possible to have both functions wrong but consistent with each other so that the test passes.
    This test nonetheless can catch many bugs and, since the gradient calculation tends to be more error prone, it's good to make sure there is no mistake there. 
    c. Plug in the now tested-and-true log-likelihood and gradient functions to `stats::optim()` to find MLE.
7. Run `devtools::test()` and check that the then-failing test from Step 2 now passes.
8. Open a pull request from your development branch to the main/master one. 
    
## Things to commit/submit

Link to the pull request.

**My answer:** Here is the link to the pull request:
