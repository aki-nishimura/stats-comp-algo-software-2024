---
title: 'Homework: Finite Precision Arithmetic & Gradient Descent'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---


```{r setup, include=FALSE}
# source(file.path("..", "R", "colors.R"))
# source(file.path("..", "R", "util.R"))

library('Matrix', 'testthat')
```

# Exercise 0: Checking your understanding of floating point numbers and finite precision arithmetic

We observed in the class that the following code returns `FALSE`:

```{r, eval=FALSE}
a <- 1 + .Machine$double.eps
b <- 1 + .Machine$double.eps 
c <- 1
(a + b) + c == a + (b + c)
```

On the other hand, the following modification results in `TRUE`:

```{r, eval=FALSE}
b <- 0.5 + .Machine$double.eps
(a + b) + c == a + (b + c)
```

Why do we get the different result?

#Answer

We get different results due to the creative difference in appromixations by the machine in either cases. The basis reasoning revolved around if the computer rounds off the epsilon values or not. 

Our guiding principle will be the fact that there are $2^{52}$ machine-representable values between two reals $2^k$ and $2^{k+1}$. In other words between 2($=2^{1}$) and 4($2^{2}$), $.Machine\$double.eps$ is not detectable but it is detectable between 1($2^{1}$) and 2($=2^{1}$). 

In more detail, let us first look at the first case, where both a and b have an extra 'epsilon' term, i.e. $a = 1 + .Machine\$double.eps$
and $b = 1 + .Machine\$double.eps$. In this case $a+b=2+2.Machine\$double.eps=2(1+.Machine\$double.eps)$, which is not rounded off as the granularity is more than the detectability of the computer. Looking at the RHS, $b = 1 + .Machine\$double.eps$ and $c=1$. In this case, $b+c=2+.Machine\$double.eps$ which is rounded off to 2. Thus the final result ends up being $(a + b) + c =3+2.Machine\$double.eps \neq a + (b + c) =3+.Machine\$double.eps \approx 3$. Hence in this case the LHS and the RHS end up not being the same.

Let us now look at the second case, where both a and b have an extra 'epsilon' term, i.e. $a = 1 + .Machine\$double.eps$
and $b = 0.5 + .Machine\$double.eps$. In this case $a+b=1.5+2.Machine\$double.eps$, which is not rounded off as the granularity is more than the detectability of the computer. Looking at the RHS, $b = 0.5 + .Machine\$double.eps$ and $c=1$. In this case, $b+c=1.5+.Machine\$double.eps$ which is again NOT rounded off to 1.5 by the above reasoning. Thus the final result ends up being $(a + b) + c =2.5+2.Machine\$double.eps = a + (b + c) =2.5+2.Machine\$double.eps $. Hence in this case the LHS and the RHS end up BEING the same.


# Exercise 1: One-pass algorithm for calculating variance

Let's say you are running a Monte Carlo simulation to estimate the mean and variance of some random variable $\boldsymbol{\theta}$. 
Imagine that $\boldsymbol{\theta}$ is high-dimensional, so you don't want to keep all the Monte Carlo samples $\boldsymbol{\theta}_i$ due to the memory burden.
(If $\boldsymbol{\theta}$ is $10^6$-dimensional, for example, storing 1,000 Monte Carlo samples would require 8GB of memory.)
And it occurs to you that you can write a "one-pass" algorithm to calculate the sample mean and sample second moment of $\boldsymbol{\theta}$ by only keeping track of running totals of Monte Carlo samples. 
You can then, in theory, calculate the sample variance from these quantities.

The above idea is implemented and applied below. 
Try it and see what you get. 
Does the estimate make sense? If not, why?
How does the estimate change if you change the seed?

**Bonus question:** 
What is, roughly speaking, the distribution of the estimator `sample_var_with_question_mark` under repeated Monte Carlo simulations (i.e. without a fixed seed)?

```{r}
set.seed(1)

running_sum <- 0
running_sum_sq <- 0

n_sample <- 10^3
for (i in 1:n_sample) {
  theta <- 1e10 + rnorm(1)
  running_sum <- running_sum + theta
  running_sum_sq <- running_sum_sq + theta^2
}
sample_mean <- running_sum / n_sample
sample_sec_mom <- running_sum_sq / n_sample

sample_var_with_question_mark <- (sample_sec_mom - (sample_mean)^2)

```

# Answer
We get a final sample variance of -147456, which is negative. The above solution does not make sense as the variance cannot be negative. Numerical instability arises due to the generation of random numbers using rnorm(1). This results in the addition of a small number to a very large number , leading to precision issues in floating-point arithmetic and causing significant loss of precision in the calculations.

Particularly, when computing running_sum_sq, squaring theta yields extremely large numbers due to the large value of theta combined with random fluctuations from rnorm(1). Summing up these large squared values over 1000 times compounds the precision issues. Consequently, the calculation of sample_var_with_question_mark is prone to substantial round-off errors, as subtracting a large squared mean from a large second moment is likely to lose significant digits of precision. 

 I ran the simulations over a large number of seeds and got the following values for the variances(shown as a histogram)-

<img src="https://github.com/achatto4/stats-comp-algo-software-2024/blob/main/homework/figure/Capture_a2.JPG?raw=true" alt="Place the screenshot here"/>

# Bonus
We see that the values are symmetrically distributed across the number line, and are almost normal in its nature. We check if it is normal and thus show the qq-plot, which is shown below-

<img src="https://github.com/achatto4/stats-comp-algo-software-2024/blob/main/homework/figure/Capturea2.1.JPG?raw=true" alt="Place the screenshot here"/>

We also ran the Shapiro-Wilk test and ontained the same inference. All the codes are implemented below.

```{r seeds}
library(ggpubr)

# Changes observed with a large number of seeds
result_stored=c()
for(i in 1:1000){
  set.seed(i)

running_sum <- 0
running_sum_sq <- 0

n_sample <- 10^3
for (i in 1:n_sample) {
  theta <- 1e10 + rnorm(1)
  running_sum <- running_sum + theta
  running_sum_sq <- running_sum_sq + theta^2
}
sample_mean <- running_sum / n_sample
sample_sec_mom <- running_sum_sq / n_sample

result_stored <- c(result_stored,(sample_sec_mom - (sample_mean)^2))
}
#histogram
hist(result_stored )

# qqplot
ggqqplot(result_stored )

#shapito test for normality
shapiro.test(result_stored )
```

## Things to commit/submit
Answers to the questions above.


# Exercise 2: Generating random real numbers in finite precision arithmetic

## Part A

You learned in the class that numeric values in computers are typically represented with 64-bits double-precision numbers, having 52-bits fraction. 
In particular, a "Uniform(1, 2)" random variable in double-precision generates a number from the $2^{52}$ unique values between $1$ and $2$.
The amount of randomness in it, therefore, is equivalent to 52 Bernoulli random variables.
Write a function to generate a Uniform(1, 2) random variable in double-precision with randomness coming only from the `rbinom()` function.
Check the function's behavior by plotting a histogram of random variables generated from it. 

```{r, imp}
d_prec_unif <- function () {
  bits <- rbinom(52, size = 1, prob = 0.5)
  weight_vec=2^-(1:52)
  fraction <- sum(bits *  weight_vec)
  result <- fraction + 1
  return(result)
}

n_draw <- 10^5
unif_rv <- sapply(1:n_draw, function (i) d_prec_unif())

rv_hist <- hist(unif_rv, breaks = 21, plot = FALSE)
plot(
  rv_hist,
  freq = FALSE,
  xlab = "Realized values",
  col = "#bdcedb", # JHU Spirit Blue with saturation and luminance adjusted: https://hslpicker.com/#bdcedb
  border = "#859bad" # Another variation of JHU Spirit Blue
)
```

# Answer
<img src="https://github.com/achatto4/stats-comp-algo-software-2024/blob/main/homework/figure/Capturea2.2.JPG?raw=true" alt="Place the screenshot here"/>

The histogram looks quite good! We also check if the data is uniform by running a Kolmogorov Smirnov Test. The Kolmogorov Smirnov test also passes. 

```{r imp1}
unif_rv<-c(unif_rv)
ks.test(unif_rv, "punif", min = 1, max = 2)
```


## Part B

As we observed above, a Uniform(1, 2) random variable in double-precision takes one of the $2^{52}$ unique values.
Two independently drawn random variables, therefore, has small but positive probability of having exactly the same value.
We can calculate the expected number of duplicate values in i.i.d. Uniform(1, 2) random variables as follows.
When drawing with replacement $d$ independent random variables out of $n$ choices, the probability of the $i$-th element never selected is $(1 - n^{-1})^d$ for $i = 1, \ldots, n$. 
The expected number of unique items drawn, therefore, is $n [1 - (1 - n^{-1})^d]$.
In turn, the expected number of duplicates is $d - n [1 - (1 - n^{-1})^d]$.
For the case of double-precision uniform random variables, we have $n = 2^{52}$ for the number of choices.

The `calc_num_expected_duplicate()` function below implements the above formula for the expected number of duplicates. 
When you call `calc_num_expected_duplicate(n_draw = 10^6, n_choice = 2^52)` to calculate the expected number for $10^6$ Uniform(1, 2) random variables, however, the function returns _exact zero_ due to a finite precision error. 
What is the issue?

# Answer- 
As described by Aki, formulas involving $1 + x$ for $|x| \ll 1$ ($x = - n^{-1}$ in the above case) often run into problems pertaining to precision. This is because When $|x|$ is very small, the addition of 
1 to $x$ can result in a number that is extremely close to 1(and so are the values obtained after taking power $n_draw$). The multiplication of a very large number with a small number to find $calc_num_expected_duplicate$ also contributeds to the same. In double-precision floating-point representation, where the significand has 52 bits, the representation of numbers very close to 
1 may lack the necessary precision to accurately capture the result of the addition. 

```{r}
calc_num_expected_duplicate <- function (n_draw, n_choice) {
  # To be remedied.
  prob_item_never_chosen <- (1 - 1 / n_choice) ^ n_draw
  n_expected_unique <- n_choice * (1 - prob_item_never_chosen)
  return(n_draw - n_expected_unique)
}
```

Formulas involving $1 + x$ for $|x| \ll 1$ ($x = - n^{-1}$ in the above case) occur commonly enough in statistics/scientific computing that R provides a function `log1p(x)` for computing $\log(1 + x)$ to deal with one such situation.
In more general calculations involving $|x| \ll 1$ (e.g. for a "sinc" funciton $\operatorname{sinc}(x) = \sin(x) / x$), a Taylor expansion provides a general approach to control truncation errors.

Remedy the `calc_num_expected_duplicate()` function by applying the second-order Taylor expansion to $(1 - x)^d$ and simplifying the formula $d - x^{-1} [1 - (1 - x)^d]$ to avoid truncation errors. 
(Remark: i) in this specific case, you could use the binomial expansion and calculate it exactly; 
ii) applying the second-order expansion to $(1 - x)^d$ corresponds to applying the first-order expansion to the overall expression $d - x^{-1} [1 - (1 - x)^d]$.)
Use the more numerically accurate version of `calc_num_expected_duplicate()` to calculate the expected number of duplicates among $10^6$ Uniform(1, 2) random variables.

```{r remedied}
calc_num_expected_duplicate <- function(n_draw, n_choice) {
  # Apply the second-order Taylor expansion to (1 - x)^d
  x <- 1 / n_choice
  d <- n_draw
  
  
  # Simplify the expression d - x^-1 * [1 - (1 - x)^d]
  result <- d*(d-1)*x/2
  
  return(result)
}

# Define parameters
n_draw <- 1e6
n_choice <- 2^52

# Calculate the expected number of duplicates
expected_duplicates <- calc_num_expected_duplicate(n_draw, n_choice)
print(expected_duplicates)

```
#Answer 

We get the expected number of duplicates to be 0.0001110222, which is quite small!

## Part C

The expected number from Part B, if you calculated it correctly, suggests that it is extremely rare to get duplicate values when generating $10^6$ uniform random variables in double-precision.
This is reassuring because we routinely generate a far larger number of random numbers in modern applications.
If we use R's `runif()` function, however, this is _not_ what we observe because [R's random numbers are generated using 32-bit integers](https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1782261).
Check the number of duplicates from `runif(10^6)` against the expected value for a floating-point number with 32-bits of precision. 
Check also that, using your implementation of random uniform from Part A, duplicates are indeed rare in double-precision.

```{r, eval=FALSE}
set.seed(1)
n_draw <- 10^6
base_r_unif <- runif(n_draw, min = 1, max = 2)
unif_rv <- sapply(1:n_draw, function (i) d_prec_unif())

#Observed duplications in 32-bit system.
sum(duplicated(base_r_unif))

# 32 bit system
n_draw <- 1e6
n_choice <- 2^32

# Calculate the expected number of duplicates
expected_duplicates <- calc_num_expected_duplicate(n_draw, n_choice)
print(expected_duplicates)

#checking number of duplications in part A implementation
d_prec_unif <- function () {
  bits <- rbinom(52, size = 1, prob = 0.5)
  weight_vec=2^-(1:52)
  fraction <- sum(bits *  weight_vec)
  result <- fraction + 1
  return(result)
}

n_draw <- 10^6
unif_rv <- sapply(1:n_draw, function (i) d_prec_unif())
sum(duplicated(unif_rv))

```
# Answer
 We get 120 replications! This is much more than that of the 52-bit scenario! We obtain the expected number of duplication to be 116 in the 32 bit system, which is quite close to the observed value of 120. Indeed for our implementation of part A the number of duplication is again 0, reinforcing our assertion that duplication in double-float systems are rare.
 
 

## Things to commit/submit
Answers to the tasks/questions above.


# Exercise 3: Numerical differentiation

## Part A: Potential caveats of numerical differentiation

You have learned in the class that the centered difference $\frac{f(x + \Delta x) - f(x - \Delta x)}{2 \Delta x}$ approximates the derivative $f'(x)$ up to an $O(\Delta x^2)$ error.
To numerically verify this, we design the following experiment using a log of logistic function (i.e. $f(x) = \log\{ \exp(x) / (1 + \exp(x)) \}$) as an example.
We evaluate its derivative using an analytical formula and compare it to the centered difference approximations as $\Delta x$ varies from $2^{-1}$ to $2^{-52}$.
Using the starter code provided below, plot the relative error of the numerical approximation in a log-log scale.
What do you expect the slope of the plotted line to be?

# Answer 

I anticipate that when plotted on a log-log scale, the resulting line will be straight, exhibiting a slope of $-2$. We know that the central difference approximation is given by:
\[
\frac{f(x + \Delta x) - f(x - \Delta x)}{2 \Delta x} \approx f'(x) + \frac{(\Delta x)^2f'''(x)}{6} = f'(x) + \Theta((\Delta x)^2)
\]
Thus, the error can be approximated as $\epsilon \approx \Theta((\Delta x)^2)$. Taking logarithm, the slope on the log-log scale is approximately $-2$.


```{r, eval=FALSE}
#' Approximate derivative via centered-difference approximation
approximate_deriv <- function(func, x, dx) {
  deriv=(func(x + dx) - func(x - dx))/(2*dx)
  return(deriv)
}

log_logistic_func <- function(x) {
   return_ans <- log(exp(x) / (1 + exp(x)))
   return(return_ans)
}

log_logistic_deriv <- function(x) {
   f_x <- 1 / (1 + exp(x))
   return_ans <- f_x 
   return(return_ans)
}


# Calculate and plot the relative errors of finite difference approximations
set.seed(615)
x <- rnorm(1)

log2_dx <- - seq(1, 52)
numerical_deriv <- sapply(
  2^log2_dx, function(dx) approximate_deriv(log_logistic_func, x, dx)
)
analytical_deriv <- log_logistic_deriv(x)
rel_err <- abs(
  (analytical_deriv - numerical_deriv) / analytical_deriv
)

fontsize <- 1.2
plot(
  log2_dx, log2(rel_err),
  frame = FALSE, # Remove the ugly box
  xlab = "log2 stepsize for finite difference",
  ylab = "log2 rel errors",
  xlim = rev(range(log2_dx)),
  cex.lab = fontsize,
  cex.axis = fontsize,
  type = 'l'
)


```
<img src="https://github.com/achatto4/stats-comp-algo-software-2024/blob/main/homework/figure/Capturea3.2.JPG?raw=true" alt="Place the screenshot here"/>


**Questions:**
Does the relative error plot agree with what the mathematical theory predicts? Explain the observed phenomenon.

Also, what is the slope in the "latter part" of your plotted line, roughly speaking? 
Explain why we observe such a slope in the relative error. 
(Hint: Consider a machine with 10 decimal digits of accuracy. What happens if we numerically differentiate $f(x) = x$ at $x = 1$ with $\Delta x = 0.1, 0.01, 0.001, \ldots$.)

# Answer

The observed relative error plot partially aligns with theoretical expectations. For larger $\Delta x$ values, it appears linear with a slope close to -2. As $\Delta x$ decreases further, the log error increasing (almost)linearly with perturbations.

Before we look at the plausible reasonings, we also draw another graph demonstrating if the relative error increases(=1) or decreases(=-1) as $dx$ gets smaller. The graph is provided below.

```{r sign }
compute_signs <- function(vec) {
  diffs <- diff(vec)
  signs <- ifelse(diffs > 0, 1, -1)
  return(signs)
}

log_logistic_func <- function(x) {
   return_ans <- log(exp(x) / (1 + exp(x)))
   return(return_ans)
}

log_logistic_deriv <- function(x) {
   f_x <- 1 / (1 + exp(x))
   return_ans <- f_x 
   return(return_ans)
}

approximate_deriv <- function(func, x, dx) {
  deriv=(func(x + dx) - func(x - dx))/(2*dx)
  return(deriv)
}

# Calculate and plot the relative errors of finite difference approximations
set.seed(615)
x <- rnorm(1)

log2_dx <- - seq(1, 52)
numerical_deriv <- sapply(
  2^log2_dx, function(dx) approximate_deriv(log_logistic_func, x, dx)
)
analytical_deriv <- log_logistic_deriv(x)
rel_err <- abs(
  (analytical_deriv - numerical_deriv) / analytical_deriv
)


fontsize <- 1.2

plot(
 log2_dx[-1],  compute_signs(log2(rel_err)),
  frame = FALSE, # Remove the ugly box
  xlab = "log2 stepsize for finite difference",
  ylab = "change in log2 rel errors",
  xlim = rev(range(log2_dx)),
  cex.lab = fontsize,
  cex.axis = fontsize,
  type = 'l'
)
```
<img src="https://github.com/achatto4/stats-comp-algo-software-2024/blob/main/homework/figure/Capturea3.11.JPG?raw=true" alt="Place the screenshot here"/>

The discrepancy between the observed relative error plot and the theoretical expectation of a straight line with a slope of -2 on a log-log scale can be attributed to the interplay of numerical precision and approximation error. Initially, for larger $\Delta x$ values, the approximation error predominates, resulting in a linear decrease in relative error with a slope close to -2, consistent with theoretical predictions. However, as $\Delta x$ decreases, numerical errors become increasingly significant relative to the approximation error. These errors stem from the finite precision of floating-point arithmetic, leading to inaccuracies in the evaluation of $f(x + \Delta x)$ and $f(x - \Delta x)$. As $\Delta x$ becomes very small, numerical issues such as round-off errors and numerical stability may further influence the relative error, deviating from the expected theoretical behavior. Therefore, while the theoretical expectation suggests a slope of -2, the presence of numerical errors and computational constraints may cause deviations from this ideal behavior in practice. The fluctuations in the log error stem from round-off errors, influenced by the least significant digits of $f(x + \Delta x) - f(x - \Delta x)$. Although the round-off error is anticipated to double with each increase in $k$, the actual error may vary, as illustrated by the accompanying sign graph. Consequently, the log error displays an upward trend interspersed with fluctuations.



## Part B: Extention to multivariate functions

As mentioned in the lecture, the finite difference method extends to a function with multivariate input $f: \mathbb{R}^d \to \mathbb{R}$ and can be used to approximate the gradient $\nabla f(\boldsymbol{x})$.
The extension relies on the fact $f(\boldsymbol{x} + \Delta x \, \boldsymbol{e}_i) = f(\boldsymbol{x}) + \Delta x \, \partial_i f(\boldsymbol{x}) + \ldots$ for $i = 1, \ldots, d$.

Using the starter code provided below, implement a function to approximate $\nabla f(\boldsymbol{x})$ via the centered difference method.
To test this function, take as an example a quadratic function $f(\boldsymbol{x}) = - \boldsymbol{x}^\intercal \boldsymbol{\Phi} \boldsymbol{x} / 2$ with positive definite $\boldsymbol{\Phi}$, which can be thought of as a log-likelihood function for $\boldsymbol{x} \sim \mathscr{N}(\boldsymbol{0}, \boldsymbol{\Phi})$ up to an additive constant.
First show that its gradient is given by $\nabla f(\boldsymbol{x}) = - \boldsymbol{\Phi} \boldsymbol{x}$; 
more generally, the gradient of a quadratic function $g(\boldsymbol{x}) = \boldsymbol{x}^\intercal \boldsymbol{A} \boldsymbol{x}$ is given by $\nabla g(\boldsymbol{x} = (\boldsymbol{A} + \boldsymbol{A}^\intercal) \boldsymbol{x}$ for any square matrix $\boldsymbol{A}$.
Then compare a numerically approximated gradient to one computed via an analytical formula.

#Answer

Consider the function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ defined as:

\[ g(\mathbf{x}) = \mathbf{x}^\intercal \mathbf{A} \mathbf{x} \]

Expanding $g(\mathbf{x} + \epsilon \mathbf{v})$, we have:

\[ g(\mathbf{x} + \epsilon \mathbf{v}) = \mathbf{x}^\intercal \mathbf{A} \mathbf{x} + \epsilon \mathbf{v}^\intercal \mathbf{A} \mathbf{x} + \epsilon \mathbf{x}^\intercal \mathbf{A} \mathbf{v} + \epsilon^2 \mathbf{v}^\intercal \mathbf{A} \mathbf{v} \]

Thus, the directional derivative of $g$ with respect to $\mathbf{v}$ at $\mathbf{x}$ is:

\[ \lim_{\epsilon \to 0} \frac{g(\mathbf{x} + \epsilon \mathbf{v}) - g(\mathbf{x})}{\epsilon} = \mathbf{v}^\intercal \mathbf{A} \mathbf{x} + \mathbf{x}^\intercal \mathbf{A} \mathbf{v} = \langle \mathbf{v}, \mathbf{A} \mathbf{x} \rangle + \langle \mathbf{A}^\intercal \mathbf{x}, \mathbf{v} \rangle = \langle \mathbf{v}, (\mathbf{A} + \mathbf{A}^\intercal) \mathbf{x} \rangle \]

Finally, the gradient of $g$ is:

\[ \nabla g(\mathbf{x}) = (\mathbf{A} + \mathbf{A}^\intercal) \mathbf{x} \]

Now in our case, we have 
\[
g(\mathbf{x} + \epsilon \mathbf{v}) 
= g(\mathbf{x}) + \frac{\partial g(\mathbf{x})}{\partial \mathbf{x}_k} \epsilon 
+ \frac{1}{2} \frac{\partial^2 g(\mathbf{x})}{\partial \mathbf{x}_k^2} \epsilon^2 
+ \frac{1}{6} \frac{\partial^3 g(\mathbf{x})}{\partial \mathbf{x}_k^3} \epsilon^3
\]

Similarly, 
\[g(\mathbf{x} - \epsilon \mathbf{v}) 
= g(\mathbf{x}) - \frac{\partial g(\mathbf{x})}{\partial \mathbf{x}_k} \epsilon 
+ \frac{1}{2} \frac{\partial^2 g(\mathbf{x})}{\partial \mathbf{x}_k^2} \epsilon^2 
- \frac{1}{6} \frac{\partial^3 g(\mathbf{x})}{\partial \mathbf{x}_k^3} \epsilon^3
\]

Combining these two, we have

\[
\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}_k} \approx \frac{g(\mathbf{x} + \epsilon \mathbf{e}_k) - g(\mathbf{x} - \epsilon \mathbf{e}_k)}{2 \epsilon} + \Theta(\epsilon^2)
\]

Let us now look at the special case of a quadratic function $f(\boldsymbol{x}) = - \boldsymbol{x}^\intercal \boldsymbol{\Phi} \boldsymbol{x} / 2$ with positive definite $\boldsymbol{\Phi}$, which can be thought of as a log-likelihood function for $\boldsymbol{x} \sim \mathscr{N}(\boldsymbol{0}, \boldsymbol{\Phi})$ up to an additive constant.

From above, the gradient of $f$ is:

\[ \nabla f(\mathbf{x}) = -(\boldsymbol{\Phi} + \boldsymbol{\Phi}^\intercal) \mathbf{x}/2 = -\boldsymbol{\Phi}\mathbf{x} \]

We make use of the fact that $\boldsymbol{\Phi}$ is symmetric.

Furthermore, Similarly as above, 

\[
\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}_k} \approx \frac{f(\mathbf{x} + \epsilon \mathbf{e}_k) - f(\mathbf{x} - \epsilon \mathbf{e}_k)}{2 \epsilon} + \Theta(\epsilon^2)
\]


```{r, eval=FALSE}
library(testthat)
approx_grad <- function(func, x, dx = .Machine$double.eps^(1/3)) {
  d <- length(x)
  numerical_grad <- numeric(d)
  for (i in 1:d) {
    x_plus <- x
    x_minus <- x
    x_plus[i] <- x_plus[i] + dx
    x_minus[i] <- x_minus[i] - dx
    numerical_grad[i] <- (func(x_plus) - func(x_minus)) / (2 * dx)
  }
  return(numerical_grad)
}

set.seed(410)
n_param <- 4
X <- matrix(rnorm(2 * n_param^2), nrow = 2 * n_param, ncol = n_param)
Sigma_inv <- t(X) %*% X
  
#' Calculate log-density of centered Gaussian up to an additive factor
gaussian_logp <- function(x, Sigma_inv) {
  logp <- - .5 * t(x) %*% Sigma_inv %*% x
  return(logp)
}

gaussian_grad <- function(x, Sigma_inv) {
  grad <- -Sigma_inv %*% x
  return(grad)
}

x <- c(3, 1, 4, 1)

are_all_close <- function(v, w, abs_tol = 1e-6, rel_tol = 1e-6) {
  abs_diff <- abs(v - w)
  are_all_within_atol <- all(abs_diff < abs_tol)
  are_all_within_rtol <- all(abs_diff < rel_tol * pmax(abs(v), abs(w)))
  return(are_all_within_atol && are_all_within_rtol)
}

analytical_grad <- c(gaussian_grad(x, Sigma_inv))
numerical_grad <- approx_grad(function(x) gaussian_logp(x, Sigma_inv), x)
T_ans=testthat::expect_true(are_all_close(
  analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
print(T_ans)
```


## Things to commit/submit
For Part A, completed code and answers to the questions.
For Part B, derivation of the gradient and completed code that passes the test.


# Exercise 4: Continuing development of `hiperglm` &mdash; finding MLE via `stats::optim()` and testing it

We will continue the development of the `hiperglm` package. 

As discussed, our plan is to first implement an MLE finder via a pre-packaged general-purpose BFGS optimizer, against which we can then use to benchmark our custom implementation of iteratively reweighted least squares. 
The idea is that using the pre-packaged function is less error-prone, especially since the gradient function is easy to unit-test.

To be extra safe, we are going to test the MLE finder via BFGS against an even simpler algorithm: the least square for a linear model via the analytical expression $\hat{\boldsymbol{\beta}}_\textrm{mle} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}$.
Complete the following steps toward this goal.
Remember that you can use [`devtools::load_all()`](https://r-pkgs.org/code.html#sec-code-load-all) to help your [package development process](https://r-pkgs.org/code.html#constant-health-checks). 

0. Make and check out (`git branch` and `git checkout`) a development branch (call it `develop`) where you will commit the work below.
1. Set up the use of `testthat`.
2. Copy the helper functions `are_all_close()` and `simulate_data()` from [the instructor's repo](https://github.com/aki-nishimura/hiperglm/blob/3706c61e1492fe8ccc77e414626d43219b65e659/tests/testthat/helper.R).
3. Test the `are_all_close()` function. 
Write a test to cover at least three cases: the function 1) correctly returns `TRUE`, 2) correctly returns `FALSE` because the relative error is above `rel_tol`, and 3) correctly returns `FALSE` because the absolute error is above `abs_tol`. 

Make sure that the logic of the test is clear and easy to understand &mdash; tests are there to clarify and validate the code's behavior; unreadable tests defeat the purpose.
<br> _Remark:_ This is just an exercise, but in general there is nothing strange about testing a function you use to test another function. It is perfectly reasonable to consider testing any functions complicated enough to potentially contain insiduous bugs.
4. Set up the (currently failing) test to compare the MLE estimated via pseudo-inverse and via BFGS. 
5. Implement the MLE finder via peudo-inverse. 
Please, pretty please, do not calculate an inverse of a matrix. 
(Otherwise, it's fine to use `solve()` instead of `chol()`.)
6. Implement the MLE finder via BFGS:
    a. Implement functions to calculate the log-likelihood and gradient under a linear model. 
    This function technically requires an estimate of the residual variance, but its value does not affect the MLE for the coefficients and hence is irrelevant for our purpose. 
    You can just specify it as an optional parameter e.g. as `function(..., noise_var = 1)`.
    b. Test your gradient calculation by comparing it against a numerical one.
    For this purpose, use the function from Exercise 2, Part B to approximate the gradient of a given function via finite difference.
    <br> _Remark:_ Technically, this test checks only for consistency between the log-likelihood and gradient function.
    It is possible to have both functions wrong but consistent with each other so that the test passes.
    This test nonetheless can catch many bugs and, since the gradient calculation tends to be more error prone, it's good to make sure there is no mistake there. 
    c. Plug in the now tested-and-true log-likelihood and gradient functions to `stats::optim()` to find MLE.
7. Run `devtools::test()` and check that the then-failing test from Step 2 now passes.
8. Open a pull request from your development branch to the main/master one. 
    
## Things to commit/submit

Link to the pull request.














