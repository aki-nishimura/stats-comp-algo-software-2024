---
title: 'Homework: Numerical linear algebra'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      bA: "{\\boldsymbol{A}}",
      bx: "{\\boldsymbol{x}}",
      bb: "{\\boldsymbol{b}}"
    }
  }
});
</script>

```{r setup, include=FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c("microbenchmark")
install_and_load_packages(required_packages)
```

# Exercise 1: Comparing different numerical linear algebra algorithms for solving linear systems

In this exercise, we consider the problem of solving a linear system $\bA \bx = \bb$ for $\bx$.
We compare the three methods we learned in the class: LU, Cholesky, and QR decompositions.
(Of course, LU applies to more general systems and QR can be used to solve least squares, but here we focus on positive definite systems.)

## Part A: Racing for solution &mdash; speed comparison 

We first compare their computational speed. 
Fill in the code below and `bench::mark()` the three algorithms.

**Questions:**
What are relative speeds among the algorithms?
Do relative speeds agree with what you expect from the complexity analyses?
If not (quite), why might that be?

**Answer**: 

*Observation*: Cholesky is the fastest in computing the linear system, followed by LU the second fastest, then solving by QR decomposition at last. 

*Explanation*: The speed agrees with complexity analysis. In complexity analysis solving linear system $Ax = b$, LU requires $\approx \frac{2}{3}p^3$ for square matrix, Cholesky requires $\approx \frac{1}{3}p^3$ for square matrix, and QR decomposition requires $\approx 2np^2$, which is $2p^3$ for square matrix $A$ here. Therefore, the cost of solving $Ax=b$ are approximately $\frac{2}{3}p^3$, $\frac{1}{3}p^3$, and $2p^3$ via LU, Cholesky, and QR respectively. So in theory, Cholesky decomposition is the fastest, followed by LU, and QR, which is exactly the order we see in this section.


```{r}
# Import the `rand_positive_def_matrix` function
source(file.path("R", "random_matrix.R"))

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
b <- rnorm(mat_size)

solve_via_cholesky <- function(A, b) {
  # Fill in
  L <- chol(A)
  Lx <- forwardsolve(L, b, transpose = T, upper.tri = TRUE)
  backsolve(L, Lx)
}

solve_via_qr <- function(A, b, tol = .Machine$double.eps) {
  QR <- qr(A, tol = tol)
  qr.solve(QR, b)
}


bench::mark(
  solve(A, b)
)
bench::mark(
  solve_via_cholesky(A, b)
)
bench::mark(
  # Solve via QR decomposition
  solve_via_qr(A, b)
  
)
```

## Part B: Competition in terms of numerical accuracy/stability

We now compare the three methods in terms of numerical accuracy/stability.
To this end, we set up the following simulation study. 
We first generate a "ground truth" solution vector $\bx_0$.
We then compute an "numerical-error-free" $\bb = \bA \bx_0$ by carrying out the matrix-vector multiplication using the `long double` type, which (on most hardware and compilers) provides [additional 12 bits of precision](https://en.wikipedia.org/wiki/Extended_precision#x86_extended_precision_format).
Of course, the vector $\bb$ computed as such still suffers from numerical errors, but the idea is that the numerical errors from this high-precision matrix-vector multiplication is much smaller than the errors caused by numerically solving $\bA \bx = \bb$ for $\bx$.
We can thus assess the accuracy of the three solvers by comparing the numerically computed $\bx$ to the ground truth $\bx_0$.

### Task &#x2F00;

First compare the outputs of matrix-vector multiplication $\bx \to \bA \bx$ using `double` and `long double` using the provided Rcpp functions.
We will discuss more about Rcpp in lecture (if we have not yet); you can for now just use the provided functions without worrying about how they work.

**Questions:**
What is the relative difference in $\ell^2$-norm? 
How about the coordinate-wise relative differences?
Are the observed magnitudes of the differences what you'd expect?

**Answer**: Relative difference in $\ell^2$-norm is 5.756881e-16. Coordinate wise difference has the following distribution. Both types of differences are small which is what I would expect, since high-precision matrix-vector multiplications are stable

```{r}
l2norm <- function(v, w) {
  sqrt(sum((v - w)^2) / sum(v^2))
}
rel_diff_coord <- function(truth, estimated) {
  abs((estimated - truth) / truth)
}
```


```{r}
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

# Fill in
source(file.path("..", "R", "colors.R"))
l2norm(matvec_ldouble(A, x), matvec_double(A, x))

rel_diff_coord_ldl <- rel_diff_coord(matvec_ldouble(A, x), matvec_double(A, x))

hist(log10(rel_diff_coord_ldl), 
     main = "Coordinate-wise Relative Difference between ldouble and double",
     xlab = "log 10 of relative difference", col = "#bdcedb", border = "#859bad")
```



### Task &#x2F06;

Now randomly generate $\bA$ so that its condition number is $10^6$.
Then solve a positive-definite system $\bA \bx = \bb$ for $\bx$ using the three algorithms and compare their outputs to the ground truth $\bx_0$.

**Questions:**
Which algorithm appears to be more accurate than the others? 
Visually demonstrate your answer.

**Answer:** 

- In terms of $\ell^2$-norm, LU is the most accurate with the least error, Cholesky the next, and the biggest error is produced by qr decomposition 
- Coordinate wise relative difference agrees with the results above that the relative error by LU is distributed more to the left (smaller error) than Cholesky and QR. Distributions of Choleksy and QR are are close to each other.


```{r}
set.seed(1918)
cond_num <- 1e6

# Larger matrices could incur substantial computational time under base R BLAS
mat_size <- 1024L 

A <- rand_positive_def_matrix(mat_size, cond_num)
x <- rnorm(mat_size)

# Fill in
x.fit.lu <- solve(A, matvec_double(A, x))
x.fit.chol <- solve_via_cholesky(A, matvec_double(A, x))
x.fit.qr <- solve_via_qr(A, matvec_double(A, x))

plot(log10(c(solve=l2norm(x, x.fit.lu),
             chol=l2norm(x, x.fit.chol),
             qr=l2norm(x, x.fit.qr))),
     type = "o", lty = 1,
     xlab = "Algorithm", ylab = "log10 of l2 norm of rel error", xaxt = "n", 
     main = "Difference between solution to \n Ax=b and ground truth")
axis(1, at=1:3, labels=c("lu", "chol", "qr"))

par(mfrow=c(2,2))
hist(log10(rel_diff_coord(x, x.fit.lu)), 
     main = "Coord-wise rel diff by LU",
     xlab = "log 10 of relative difference", col = "#bdcedb", border = "#859bad")
hist(log10(rel_diff_coord(x, x.fit.chol)), 
     main = "Coord-wise rel diff by Cholesky",
     xlab = "log 10 of relative difference", col = "#bdcedb", border = "#859bad")
hist(log10(rel_diff_coord(x, x.fit.qr)), 
     main = "Coord-wise rel diff by QR",
     xlab = "log 10 of relative difference", col = "#bdcedb", border = "#859bad")
```


### Task &#x4E09;

In Task &#x2F06;, we compared the three algorithms in one randomly generated example.
Now we consider a more systematic (though hardly comprehensive) comparison via repeated simulations.
We also vary the condition number of $\bA$ and assess whether the results would hold across varying degrees of ill-conditioning.

**Questions/To-do's:**

* Using the starter code provided, calculate various summary measures of the numerical errors.
* Integrate into the provided code one another (or more, if you like) meaningful metric(s) of your choice to summarize the numerical error.
* Visually explore how the three algorithms compare with each other in their accuracy. See if different error metrics tell different stories; they might or might not.
* Vary the condition number in the range $10^6 \sim 10^{12}$, e.g. by trying $10^6$, $10^9$, and $10^{12}$.
* Do you see any patterns in the numerical errors across the three algorithms, metrics, and/or condition numbers? Show some plots to support your conclusion.

**Note:** 
The QR solver will throw an error when the system is ill-condition enough that the numerical solution might not be very accurate. 
To force it to return the solution in any case, set `tol = .Machine$double.eps`.


**Answer**:

*Observations*:

- Performing 32 simulations for each decomposition method and each condition, we observe that Cholesky is the most stable, followed by LU. QR is the least stable. Similar pattern is observed using almost all metrics and all condition number
  - Under condition number 1e6 and 1e12, Choleksy is the most stable for all metrics, with the density of error distributed to the left most, followed by LU's density in the middle, and QR more to the right
  - Under condition number 1e9, the above pattern is seen in norm, median, and 5 percentile, but in 95 percentile and l1norm, distribution of Cholesky and LU almost overlaps, while QR is still on the right most. But even though LU and Choleksy's 95 percentile and l1norm error distribution overlaps more than in the other metrics and other condition number, Cholesky's 95 percentile and l1norm error have less variance than LU's, based on the plot; The mode of Cholesky's 95 percentile and l1norm error also has higher density than LU's. Therefore, it is still safe to say that Cholesky is the most table.
- Under all metrics (norm, medium, 5 percentile, 95 percentile, l1norm), and for all three algorithms, the higher the condition number, the higher the errors

*Explanation*: The observation pattern agrees with the theory. 

- Under the same condition number
  - In Cholesky: $L_{mach}L_{mach}^T = A + \Delta A$. For $L = UD V^T$ using SVD decomposition, $D=Diag(d_i)$ where $d_i$ are decreasing eigenvalues of $L$, $A=LL^T = UD V^T VD U^T = UD^2U^T$, so eigenvalues of $A$ are squares of eigenvalues of $L$, therefore $||A|| = ||L||^2 = ||L||\, ||L^T||$, $\frac{||\Delta A||}{||A||} =\frac{||\Delta A||}{||L||\,||L^T||}= O(\epsilon_{mach})$. Cholesky satisfies that $(A+\Delta A)x_{mach} = b$ for $\frac{||\Delta A||}{||A||} = O(\epsilon_{mach})$, thus $||x_{mach} - A^{-1}b|| = O(k(A)\epsilon_{mach})$
  - In LU, $L_{mach}U_{mach} = A + \Delta A$, $\frac{||\Delta A||}{||L||\,||U||} = O(\epsilon_{mach})$, but $||L||\,||U|| > ||A||$, resulting in big $\Delta A$. Therefore, the numeric solution $x_{mach}$ of $Ax=b$ via LU does not satisfy $(A+\Delta A)x_{mach} = b$ for $\frac{||\Delta A||}{||A||} = O(\epsilon_{mach})$. LU is thus less stable than Cholesky.
  - In QR, $Ax = QRx= b$, $x = R^{-1}Q^{-1}b = R^{-1}Q^{T}b$ since $Q$ is orthogonal. Thus the error $||x_{mach} - A^{-1}b|| = O\left(\left[k(A) + \frac{||Ax_{mach} - b||}{||Ax_{mach}||}k^2(A) \right]\epsilon_{mach}\right)$, where $k^2(A)$ term is small when $||Ax_{mach} - b||$ is small. Solving via QR is thus less stable than Cholesky, which agrees with the plots below
- And since the error are all roughly $O(k(A)\epsilon_{mach})$, it makes sense that the higher the condition number, the larger error we get, regardless of algorithms


```{r}
# Utility functions for bookkeeping simulation results.
source(file.path("R", "num_linalg_sim_study_helper.R"))

n_sim <- 32L
mat_size <- 512L
cond_num <- 1e6
metrics <- c("norm", "median", "five_percentile", "ninety_five_percentile", "l1norm")
  # TODO: add another metric and modify the helper script accordingly

simulate <- function(cond_num, n_sim, mat_size, metrics) {
  rel_error_list <- lapply(
    c("lu", "chol", "qr"), function(method) pre_allocate_error_list(n_sim, metrics))
  
  for (sim_index in 1:n_sim) {
    A <- rand_positive_def_matrix(mat_size, cond_num)
    x <- rnorm(mat_size) 
    b <- matvec_ldouble(A, x)
    x_approx <- list( 
      # Fill in
      lu=solve(A, matvec_double(A, x)),
      chol=solve_via_cholesky(A, matvec_double(A, x)),
      qr=solve_via_qr(A, matvec_double(A, x), tol = .Machine$double.eps)
    )
    for (method in c("lu", "chol", "qr")) {
      rel_error <- lapply(
        metrics, 
        function (metric) calc_rel_error(x, x_approx[[method]], metric)
      )
      names(rel_error) <- metrics
      for (metric in names(rel_error)) {
        rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
      }
    }
  }
  rel_error_list
}


# TODO: visually compare errors
cond_num <- 1e6
rel_error_list_6 <- simulate(cond_num, n_sim, mat_size, metrics)
cond_num <- 1e9
rel_error_list_9 <- simulate(cond_num, n_sim, mat_size, metrics)
cond_num <- 1e12
rel_error_list_12 <- simulate(cond_num, n_sim, mat_size, metrics)
```



```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(ggpubr)

make_error_plot <- function(data, metric, cond_num) {
  data.frame(method = rep(c("LU", "Chol", "QR"), rep(32, 3)),
                    rel_err = c(data[["lu"]][[metric]], 
                                data[["chol"]][[metric]], 
                                data[["qr"]][[metric]])) %>% 
    ggplot( aes(x=log10(rel_err), fill=method)) +
  geom_density( color="#e9ecef", alpha=0.6, position = 'identity') +
  labs(fill="") + xlab("log10 of relative error")+ggtitle(gsub("_", " ", metric), subtitle = paste("k=", cond_num)) + theme(plot.title = element_text(size=12), plot.subtitle = element_text(size = 10), axis.title = element_text(size = 10))
}


ggarrange(make_error_plot(rel_error_list_6, "norm", 1e6), 
          make_error_plot(rel_error_list_6, "median", 1e6), 
          make_error_plot(rel_error_list_6, "five_percentile", 1e6), 
          make_error_plot(rel_error_list_6, "ninety_five_percentile", 1e6),
          make_error_plot(rel_error_list_6, "l1norm", 1e6),
          ncol = 3, nrow = 2, common.legend = TRUE, legend="bottom")

ggarrange(make_error_plot(rel_error_list_9, "norm", 1e9), 
          make_error_plot(rel_error_list_9, "median", 1e9), 
          make_error_plot(rel_error_list_9, "five_percentile", 1e9), 
          make_error_plot(rel_error_list_9, "ninety_five_percentile", 1e9),
          make_error_plot(rel_error_list_9, "l1norm", 1e9),
          ncol = 3, nrow = 2, common.legend = TRUE, legend="bottom")

ggarrange(make_error_plot(rel_error_list_12, "norm", 1e12), 
          make_error_plot(rel_error_list_12, "median", 1e12), 
          make_error_plot(rel_error_list_12, "five_percentile", 1e12), 
          make_error_plot(rel_error_list_12, "ninety_five_percentile", 1e12),
          make_error_plot(rel_error_list_12, "l1norm", 1e12),
          ncol = 3, nrow = 2, common.legend = TRUE, legend="bottom")
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      X: "{\\boldsymbol{X}}",
      y: "{\\boldsymbol{y}}",
      bm: ["{\\boldsymbol #1}", 1]
    }
  }
});
</script>

# Exercise 2: Continuing development of `hiperglm` &mdash; improving numerical stability of Newton's method via QR-based solver

From the previous assignments, you should now have a `hiperglm` package that implement an MLE finder via Newton's method. 
We have so far focused on getting the functionality correct, and not so much on numerical stability and computational speed.
In this assignment, you improve the algorithm's numerical stability by turning Newton's method into the mathematically equivalent iteratively re-weighted least squares (IWLS) based on QR decomposition.
(And, in next assignment, you will improve the computational speed by interfacing the package with RcppEigen.)

Much of this and next assignments is about enhancing performance of the package's existing features, rather than adding new ones.
This is one setting in which the formal automated testing really proves its value. 
By running the existing tests after each change, you can be confident that you did not break anything by changing the internal implementations that do not affect the interface.
And, if you did change functions' interfaces, the tests help you remember where you have to make corresponding changes to ensure correct overall functionality.

Before you start this assignment, remember to first address all the feedback from the previous assignment and merge your previous work to the main branch. 
Then create a new `irls` branch from there, check it out, and commit all your work for this assignment there.
Finally, open a pull request into your main branch after completing the assignment and request a review from the TA.

1. Change your code for fitting linear models to solve the least squares problem using QR decomposition instead of LU/Cholesky via the pseudo-inverse formula.
Make sure _not_ to compute the "Q" matrix explicitly as it is unnecesssary and computationally expensive to do so.
Remember to run the existing tests to make sure the change did not break anything.
2. Implement the `vcov()` S3 method for linear model by inverting the estimated Fisher info $\hat{\sigma}^2 \X^\intercal \X$ for $\hat{\sigma}^2 = \frac{n}{n - p} \| \y - \X \bm{\beta} \|^2$ where $n$ denotes the sample size and $p$ the number of predictors.
(As discussed in class, this is an extremely rare situation in which it actually makes sense to explicitly calculate the inverse of a matrix.)
Calculate the inverse from the "R" factor of $\X$ and `chol2inv()`; see the tip below for extracting the "R" factor from the `qr()` output.
3. Turn, if you have not already done so, each iteration of Newton's method into an isolated function.
In other words, write a function named something like `take_one_newton_step(...)` and use it within the for-loop.
This in particular facilitate unit-testing one step of Newton's method.
4. Viewing the vector $\boldsymbol{H}_f^{-1} \nabla f$ in the Newton update formula as the solution of a weighted least squares problem, implement the solver for it based on QR decomposition.
To do so, observe that minimizing $\| \bm{W}^{1/2} (\X \bm{\beta} - \y) \|^2$ is equivalent to minimizing $\| \tilde{\X} \bm{\beta} - \tilde{\y} \|^2$ with the modified input $\tilde{\X}$ and $\tilde{\y}$. 
Use base R linear algebra functions for now.
Add an option to the function `take_one_newton_step(...)` (or however else you decided to call it) to switch between the QR- and pseudo-inverse-based solvers, with QR being default.
Test that the two solvers yield the same result up to finite precision errors.
5. Implement the `vcov()` method for logistic model by inverting the observed Fisher information at the MLE.
As before, calculate the inverse from the "R" factor of the QR decomposition.
6. Test that the package produces the coefficient and uncertainty estimates with right statistical properties.
Specifically, repeatedly apply `hiper_glm()` to randomly generated data, construct confidence intervals from `coef()` and `vcov()`, and compare the intervals' coverage rate with nominal one.
Use a "simultaneous" confidence interval of the form $\big\{ \bm{\beta} : \big\| \hat{\bm{\Sigma}}^{-1/2} \big( \bm{\beta} - \hat{\bm{\beta}} \big) \big\|^2 < q \big\}$ with $\hat{\bm{\beta}}$ and $\hat{\bm{\Sigma}}$ being the outputs of `coef()` and `vcov()`;
as for the multiplication by $\hat{\bm{\Sigma}}^{-1/2}$, use a Cholesky-based method to achieve the same effect.
Choose $q$ according to the target nominal coverage of your choice.
Make sure to test the estimates under both linear and logistic models.
    * When testing statistical properties, what constitutes "pass" and "fail" is not as clear-cut as when testing deterministic behaviors.
You should, however, still strive to choose a criteria as rigorously justifiable as possible; 
see the note below for a suggestion.
Make the logic behind your pass-fail criteria as self-explanatory as possible in code (e.g. through expressive variable/helper function names) and, where further explanation is warranted, add it as inline comments.
7. Incorporate the above test of the package's statistical behavior as a formal test under the `tests/testthat` folder but, since the test takes a while to run and its result is not completely clear-cut, make it optional using the `testthat::skip_if_not()` function.
For example, you can check the condition `Sys.getenv("MANUAL_TEST") == ""`;
the test then is run only when you set the environmental variable `MANUAL_TEST` to a non-empty string, e.g. via `Sys.setenv(MANUAL_TEST = TRUE)`.
    * Keeping it as an optional test not only allow you to re-run it as warranted (e.g. when switching to different methods for quantifying uncertainty) but also serve as a record of you having tested the package behavior, which otherwise would not be visible to others reading your code.
(On the other hand, when a test --- or any piece of code for that matter --- becomes obsolete, just get rid of it.
Even in the unlikely case you ever need it back, you can always find it in the git history.)

**Tip on Step 2 and 5:**
Use the following code snippet to extract the "R" factor from the output of the `qr()` function.
The row re-ordering is required because the function applies the QR algorithm with _column pivoting_, permutation of the input matrix columns to improve numerical stability.
```{r, eval=FALSE}
qr_decomp <- qr(X)
R <- qr.R(qr_decomp)
R <- R[ , order(qr_decomp$pivot)]
are_all_close(X, qr.Q(qr_decomp) %*% R) # Returns `TRUE`
```

**Note on Step 6:** 
When using Monte Carlo simulation to evaluate statistical performance, the output necessarily deviates from the exact expected value due to random errors.
To a great extent, however, it is possible to quantify magnitude of these random errors and correctly account for it.
For instance, let's assume that we have confidence intervals with exact nominal coverage $r \in (0, 1)$.
Under $m$ replications, the empirical coverage rate lies within $r \pm 1.96 * \hat{\sigma}$ for $\hat{\sigma}^2 = r (1 - r) / m$ with approximately 95% probability.
Using this fact, we can construct a test that succeeds 95% of the time in correctly declaring the empirical coverage to be consistent with the nominal coverage.
In practice, you would probably like to allow for a larger deviation from the nominal coverage when checking code's correctness since the implemented statistical method itself isn't perfect, with the `hiper_glm`'s `vcov()` in particular relying on asymptotic approximation.
Accounting for this additional source of error may require some guesstimation and manual tweaking.
Nonetheless, reasoning as above helps you quantify how much deviation from the nominal coverage is reasonably explained by Monte Carlo errors and when the deviation points to an issue in implementation.

## Things to commit/submit

* Link to the pull request: https://github.com/yanxiliu230/hiperglm/pull/6
